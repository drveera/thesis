%Section from Robin Gilbert's Thesis to get formatting correct.
\documentclass[oneside,12pt]{memoir}
\usepackage{mathptmx}  
\DoubleSpacing         
\usepackage{pwasu}  
%End Robin's section   

\bibliographystyle{plain}


\newcommand{\tab}{\hspace*{2em}}
\usepackage[english]{babel}
\usepackage{amssymb,amsmath,graphicx,mdwlist,paralist,multirow,array,color}
\usepackage[noend]{algpseudocode}
\usepackage{chngpage,url,algorithm,algorithmicx,wrapfig}

\renewcommand{\algorithmicforall}{\textbf{for each}}
%\algrenewcommand{\algorithmiccomment}[1]{\hfill$\blacktriangleright$ #1}

\DoubleSpacing

\begin{document}

\maxtocdepth{subparagraph} % put everything into the ToC
\pagestyle{plain}  % pagestyle for the prelims
\frontmatter
\thetitlepage

\setcounter{page}{1}
\asuabstract 
It is commonly known that High Performance Computing (HPC) systems are most frequently used by multiple users for batch job, parallel computations. Less well known, however, are the numerous HPC systems servicing such data so sensitive that administrators enforce either \begin{inparaenum}[\itshape a\upshape)] 
\item sequential job processing - only one job at a time on the entire system, or
\item physical separation - devoting an entire HPC system to a single project until recommissioned.
\end{inparaenum}
The driving forces behind this type of security are numerous but share the common origin of data so sensitive that measures above and beyond industry standard are used to ensure information security. This paper presents a network security solution that provides information security above and beyond industry standard, yet still enabling multi-user computations on the system. This paper's main contribution is a mechanism designed to enforce high level time division multiplexing of network access according to security groups. By dividing network access into time windows, interactions between applications over the network can be prevented in an easily verifiable way. 

\asuacknowledgements
{I would like to acknowledge . . .}

	\newpage
\vspace*{\fill}
\begingroup
\hspace*{\fill}
\centering{\textit{To my wife for her unwavering support}}
\hspace*{\fill}
\endgroup
\vspace*{\fill}

\tableofcontents

\listoffigures

\mainmatter
\pagestyle{asu} 

\addtocontents{toc}{
 \protect\noindent CHAPTER \protect\par

}
\chapter{Introduction}
High Performance Computing (HPC) systems are comprised of numerous individual computing systems networked and administrated together such that they can be used as a single system. Popular examples of these systems include some custom made such as the Cray I and the modern IBM Sequoia~\cite{leavitt2012big}, though more common are simple Computer Clusters in which Commercial Off The Shelf (COTS) equipment is utilized~\cite{buyya1999high}. Security within these systems is often enforced through traditional Operating System (OS) mechanisms of memory protection and file access privileges~\cite{buyya1999high}. 



Application developers for this domain of systems spans a broad spectrum, ranging from undergraduate students learning concurrent programming to defence contractors executing classified simulations. Moving towards the most demanding end of the spectrum, security concerns among application-side stakeholders increase and additional methods are employed to enforce information security. At some point along this spectrum stakeholders decide on physical separation to satisfy security concerns. The reasons behind this can be numerous, but likely stem from two major goals: simplicity of implementation and verification; and risk aversion/management. It is undeniable that physical separation provides a level of information security that is hard to replicate through the use of software, however the financial costs are significant - devoting entire HPC systems to running jobs sequentially. 


This paper presents Time Division Multiplexing (TDM) of network access as a viable alternative to physical separation. By modulating network access between application security groups we can provide an intuitive security mechanism capable of being verified in real-time. Furthermore by implementing this mechanism at the operating system level it becomes transparent to user applications, meaning no modification to existing application code is necessary. 
\chapter{Related Work}
The problem statement and proposed solution represent the intersection of two somewhat disparate fields - Time Division Multiplexing and High Performance Computing Security. Related works are therefore divided between the two. 
\subsection{High Performance Computing Security}
The size and cost of HPC environments dictates that each system is somewhat unique. The security solutions implemented within each are similarly unique. Sandholm et al.~\cite{sandholm2004ogsa} make an attempt at rectifying this larger problem by creating a framework that uses automates user access permissions and resource allocation using "XACML (eXtensible Access Control Markup Language)". They further extend their solution by tying it in to existing job submission tools (Globus Toolkit~\cite{globus_toolkit} and NorduGrid~\cite{nordugrid}). 
\subsection{Time Division Multiplexing}

\chapter{Problem Definition}
We begin by defining an abstract HPC environment through which the general case of our security challenge is shown. In this section we provide brief descriptions of the major resources common to most HPC systems. Furthermore, to design our mechanism, certain assumptions must be made on how each resources is operated. 

\subsection{Environmental Assumptions}
\begin{figure}
\centering
\includegraphics[scale=0.5]{abstract_hpc_environment.pdf}
\caption{An abstract HPC environment.}
\label{fig:abstract_hpc_environment}
\end{figure}
There are four basic resources in most HPC systems represented in Figure~\ref{fig:abstract_hpc_environment} as \begin{inparaenum}[\itshape a\upshape)]
\item compute nodes, \item persistent storage, \item administrative nodes, \item network infrastructure. Worth consideration also is the process of job allocation and the execution of jobs.\end{inparaenum}
\subsubsection{Compute Nodes}
Compute nodes are independent computing devices designated to run user submitted applications. These devices are capable of storing temporary data locally. They send and receive data across network infrastructure for three main purposes:\begin{inparaenum}[\itshape a\upshape)]
\item storing or accessing data on the persistent storage devices;
\item relaying data between other compute nodes working in tandem on the same user application;
\item and sending or receiving commands (or reports, as the case may be) from the administrative nodes, through which users interact.\end{inparaenum} 

It is assumed that these compute nodes do not co-locate user applications (e.g., two different user applications are running in the same system memory) and that user applications are not given administrative access at this level. No assumption is made about the use of virtual machines on compute nodes.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.28\textwidth]{inline_media_encryptor.jpg}
  \end{center}
  \caption{The $KG-200$ Inline Media Encryptor, certified by the NSA for use in securing persistent storage~\cite{nsa_ime_website}.}
\label{fig:inline_media_encryptor}
\end{figure}

\subsubsection{Persistent Storage}
Persistent storage devices are capable of storing large quantities of user application data, and are usually of much higher capacity than the compute nodes. User data is commonly co-located across disks using RAID (redundant array of inexpensive disks) technology ~\cite{katz1989disk} for higher storage efficiency and redundancy. 

It is assumed that Inline Media Encryptors (IMEs) and Posix permissions are used to enforce data access rules within persistent storage~\cite{nsa_ime}. IMEs have been certified for use in classified networks by the U.S. National Security Administration since 2006~\cite{nsa_ime_website}. 

\subsubsection{Administrative Nodes}
Administrative nodes are computing devices where \begin{inparaenum}[\itshape a\upshape)]
\item both administrators and users interact with the system, common tasks of which include issuing job or system commands, accessing reports and results, and performing maintenance;
\item resource management software is centrally located and executed ~\cite{keller2001anatomy}, common examples include IBM's Tivoli Workload Scheduler and the MOAB Cluster Suite by Adaptive Computing \cite{arackal2009access}\cite{jackson2006demand}.
\end{inparaenum}

It is assumed that the scheduler located here is capable of providing access to the list of current running applications and the hardware resources devoted to them. 
\subsubsection{Network Infrastructure}
Network infrastructure devices facilitate the transmission of data between nodes within the HPC system. Mediums vary widely and include copper, optical, and wireless. The most common technologies used in HPC environments are Ethernet and InfiniBand \cite{bozzo2006design}\cite{madai2010performance}. 

It is assumed that the network infrastructure uses Internet Protocol to communicate among nodes.

\subsubsection{Job Execution}
Best practices for developing jobs run on HPC systems dictates the minimization of I/O, both to disk and over the network ~\cite{shan2007using}. This I/O minimization is due to the dramatic increase in access time as data moves further away from the CPU and main memory. It's over 50 times more costly to access 1MB of data from the network than it is from main memory ~\cite{dean2009designs}. This overhead increases to almost a factor of 100 if that data is initially read from disk then sent over the network ~\cite{dean2009designs}.

In the effort to minimize the cost of I/O transactions, previous researchers have shown that batching I/O into larger transactions can reduce overhead~\cite{shan2008characterizing}. The difference between sequentially reading 1K files from network disks and reading 256MB from network disks shows a factor of 1700 improved performance by reading in larger batches~\cite{shan2007using}. The batching of I/O, especially the most costly forms (disk and network) is therefore considered best practice when possible~\cite{borrill2007investigation}. It is therefore assumed that job developers will attempt to maximize I/O batching, the optimal case of which would have an I/O transaction history similar to that shown in Figure \ref{fig:application_model}.

\begin{table}
	\begin{center}
	    \begin{tabular}{  l | r }
	    Action & Time to Complete \\ \hline
	    L1 cache reference & 0.5 ns  \\ 
	    L2 cache reference & 7 ns  \\
	    Main memory reference & 100 ns \\
	    Read 1 MB sequentially from memory & 250,000 ns  \\  
	    Read 1 MB sequentially from network & 10,000,000 ns \\ 
		Read 1 MB sequentially from disk & 30,000,000 ns \\
	    \end{tabular}
	\end{center}
	\caption{Access time examples showing the magnitude of difference between data over I/O and data locally stored~\cite{dean2009designs}.}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{application_model.pdf}
  \end{center}
  \caption{The assumed model of application execution in an HPC environment. $\alpha_{start}$ and $\alpha_{end}$ are periods where execution is I/O bound, and $\varepsilon$ is the prominent period where execution is CPU bound. This structure adheres to research showing batched I/O minimizes the I/O cost in terms of time.}
\label{fig:application_model}
\end{figure}

\subsection{Security Challenge}

\begin{table}
\begin{adjustwidth}{-4em}{-4em}
\centering
\begin{tabular}{ccc|c|c|}
& & \multicolumn{3}{ c }{\textbf{Solutions}} \\ \cline{3-5}
\centering\arraybackslash \textbf{Hardware Location} &  \multicolumn{1}{ c| }{\centering\arraybackslash \textbf{Security Challenge}} & \multicolumn{1}{ b{1.65cm}| }{\centering\arraybackslash Board Separation} & \multicolumn{1}{ b{.6cm}| }{\centering\arraybackslash IME} & \multicolumn{1}{ b{1.97cm}| }{\centering\arraybackslash Posix \hspace{2em}Permissions}   \\ \cline{1-5}
\multicolumn{1}{ |c| }{\multirow{2}{*}{Compute Nodes} } &
\multicolumn{1}{ |p{5cm}| }{\centering\arraybackslash Local Data Storage} & \checkmark &   & \checkmark         \\ \cline{2-5}
\multicolumn{1}{ |c  }{}                        &
\multicolumn{1}{ |p{5cm}| }{\centering\arraybackslash Local Data Processing} & \checkmark &   &  \checkmark        \\ \cline{1-5}
\multicolumn{1}{ |c|  }{Persistent Storage}  &
\multicolumn{1}{ |p{5cm}| }{\centering\arraybackslash Co-location of user data} &  &  \checkmark & \checkmark   \\ \cline{1-5}
\multicolumn{1}{ |c|  }{Administrative Node}  &
\multicolumn{1}{ |p{5cm}| }{\centering\arraybackslash Accept and Schedule User Jobs} &   &   & \checkmark   \\ \cline{1-5}
\multicolumn{1}{ |c|  }{\multirow{6}{*}{Network Infrastructure}} &
\multicolumn{1}{ |p{5cm}| }{\centering\arraybackslash Transmit intra-application data between compute nodes} &   &   &       \\ \cline{2-5}
\multicolumn{1}{ |c  }{}                        &
\multicolumn{1}{ |p{5cm}| }{\centering\arraybackslash Transmit data to/from compute nodes and persistent storage} &   &   &       \\ \cline{2-5}
\multicolumn{1}{ |c  }{}                        &
\multicolumn{1}{ |p{5cm}| }{\centering\arraybackslash Transmit commands from scheduler to compute nodes} &   &   &       \\ \cline{1-5}
\end{tabular}
\caption{Security challenges and technology used to solve them}
\label{tab:security_challenges}
\end{adjustwidth}
\end{table}

The security fear of customers with sensitive data is that a different customer could, through chance or intention, acquire or manipulate their data. There exist three main mediums across which data may be exposed. 
\chapter{Design Goals}
Here we describe the major goals in the design and implementation of the TDM security mechanism.
\subsection{Mechanism and Policy Separation}
Envisioning the tool as enabling a more secure form of HPCaaS, the type, number, and scale of jobs assigned to any of the systems can be widely varied. To handle this, at the least, the tool must be capable of receiving and modifying policy at the start of each new job. Furthermore, to provide more fine-grained control an online policy modification scheme is desirable for managing performance tradeoffs between security groups. We define the mechanism and policy split of our tool to be an ability to receive and actuate policy changes at the finest granularity in which the tool operates. [Mechanism Policy split is really along the difference between the formal notion that nodes must be transparently separated from the medium and how it is accomplished (with a queue, at the TCP layer, on ethernet, etc.)]
\subsection{Network Fabric Agnostic}
Two major technologies are used to network HPC systems: Ethernet and InfiniBand. Any tool for improving security across the broad spectrum of HPC systems must be capable of operating in each. Further, numerous network topologies exist within these technologies; switched fabric and tree structures are the most common for InfiniBand and Ethernet, respectively. For our purposes we define this tool to be network fabric agnostic if the tool is conceptually capable of being implemented in either Ethernet or InfiniBand networks. 
\subsection{User Application Transparent}
A fundamental requirement of the tool is that it be transparent to user applications. Applications written for HPC environments are often quite complex and it is likely that customers would be reluctant to make even minor modifications, especially to programs written in the past that are under re-use. For our purposes we define user application transparency as the ability to run an application without modification to successful end on an HPC system using our tool, given that it can also do so on a system not using our tool. 
\chapter{Formal Definition}

Suppose \emph{S} is a finite set of security groups, s.t. each security group $s\in S$ is made up of a number of compute nodes. Given \emph{S}, the language our mechanism operates on can be generated by the following grammar. Because the language is dependent on the security groups \emph{S}, this grammar must be generated based on it. This is done in two steps:

First, we define the base grammar:\\

\tab $G^1 = (V^1, \Sigma^1, R^1, \mathcal{A})$, where

\tab $V^1 = \{\mathcal{A} , W\}$ \hfill non-terminal symbols \hspace*{6em}

\tab $\Sigma ^1 = \{ \emptyset\}$ \hfill terminal symbols\hspace*{6.3em}

\tab $R^1 = \lbrace \hspace*{1.35em}\mathcal{A} \rightarrow \varepsilon ,$ \hfill rules of production \hspace*{6em} \\
$\tab \hspace*{5.9em} \mathcal{A}\rightarrow W\mathcal{A} |W\rbrace$\\

This base grammar, through the non-terminal symbols and production rules, establishes a means of generating the base language form of unordered windows ($W\in V^1$) in an arbitrary length such as $WW$ or $WWWWW$.

Next, we generate the \emph{S} specific definitions. To do so it is first necessary to define notation for two special terminal symbols and three special sets:\\
\\
$o_{s,i}$ - an open command issued to node $i$ within security group $s$,\\
$a_{s,i}$ - an acknowledgement received from node $i$ within security group $s$,\\
$\theta_s$ - the set of all $o_{s,i}$ terminals for security group $s$, \\
$\alpha_s$ - the set of all $a_{s,i}$ terminals for security group $s$, and \\
$\pi(A)$ - the set of all permutations of the set A.\\

These definitions allow us to define a final, special set:\\
\\
$\Lambda_s = \pi(\theta_s) \times \pi(\alpha_s)$\\

Intuitively, $\Lambda_s$ is a set of ordered sets expressing each permutation of $\theta_s$ matched with each permutation of $\alpha_s$. For example, given a security group $s$ made up of two elements s.t. $s = \{1, 2\}$, $\Lambda_s$ is defined:\\
\\
$\Lambda_s = \{ (o_{s,1}, o_{s,2}, a_{s,1}, a_{s,2}),\hspace*{1.0em} (o_{s,2}, o_{s,1}, a_{s,1}, a_{s,2}),$\\
\hspace*{3.0em}$ (o_{s,1}, o_{s,2}, a_{s,2}, a_{s,1}),\hspace*{1.0em} (o_{s,2}, o_{s,1}, a_{s,2}, a_{s,1})\}$\\

The sets within $\Lambda_s$ represent all legitimate command sequences within a window ($W$) for security group $s$. The key property of the sets within $\Lambda_s$ is that each node within the security group is issued an open command, in any order, followed by acknowledgements from each node within the security group, once again in any order.

With these definitions established we can now formally define an $\emph{S}$ specific grammar:

\tab $G^2 = (V^2, \Sigma^2, R^2, \emptyset)$, where

\tab $V^2 =\{W\}$

\tab $\Sigma ^2 = \{[o_{s,i}, a_{s,i}]:\forall i \in \forall s\in \emph{S}\}$

\tab $R^2 = \{[W\rightarrow \lambda  ] :\forall \lambda \in \Lambda_s :\forall s\in \emph{S}\}$


These definitions add new terminal symbols and the necessary production rules to generate them. Note the use of $\Lambda_s$ in the production rules. These rules provide every possible command sequence possible for any window $W$ s.t. every node issued an open command is required to report back with an acknowledgement before continuation onto another window.

Finally, the language our mechanism accepts for security group \emph{S} can be formed using the union of the previous two grammars:\\

\tab $G = (V, \Sigma , R, \mathcal{A})$, where

\tab $V = V^1\cup V^2$

\tab $\Sigma = \Sigma ^1\cup \Sigma ^2$

\tab $R = R^1\cup R^2$
\chapter{Implementation}
As a proof of concept we have implemented a version of the tool for the Linux operating system using C++11. In this section we will describe the tool's architecture, operation, and how it adheres to the design goals from the previous section.
\subsection{Overview}
\begin{algorithm}
	\caption{Window Controller opening and closing network access windows.}
	\label{alg:window_controller}
	\begin{algorithmic}[1]
	\Statex
	\Function{$Open\_Windows$}{$Scheduler$}
		\State $Scheduler.initialize();$
		\While{$End\_Command\_Not\_Received$}
			\State $Security\_Group \gets Scheduler.get\_next\_group();$
			\State $Security\_Group.state \gets \textsc{state.open};$
			\ForAll {$node \in Security\_Group$}
				\State $send(node.address,$ \par
	        \hskip\algorithmicindent\hspace{3em} $Security\_Group.crypto\_sign(\textsc{command.open}));$
        \State $node.state \gets \textsc{state.open};$
			\EndFor
			\While{$Security\_Group.state == \textsc{state.open}$}
				\State $node\_response \gets block\_on\_receive\_message();$
				\If{$node\_response.state == \textsc{state.closed}$} 
					\State $node.state \gets \textsc{state.closed};$
				\Else 
					\State $\textbf{throw}\hspace{.6em} \textsc{error.unclosed\_node};$
				\EndIf
				\State $Security\_Group.state \gets \textsc{state.closed};$
				\ForAll {$node \in Security\_Group$}
					\If{$node.state == \textsc{state.open}$}
						\State $Security\_Group.state \gets \textsc{state.open};$
					\EndIf
				\EndFor
				
			\EndWhile
		\EndWhile

	\EndFunction

	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Node Control Mechanism opening and closing access to the network.}
	\label{alg:node_controller}
	\begin{algorithmic}[1]
		\Statex
		\Function{$Node\_Control\_Mechanism$}{}
			\State $Queue \gets initialize\_queue;$
			\While{$exit\_command\_not\_received$}
				\State $state \gets Close\_Network\_Access(Queue);$
				\While {$open\_command\_not\_received$}
					\State $message \gets block\_on\_receive\_message();$
				\EndWhile
				\State $state \gets Open\_Network\_Access(Queue);$
				\State $sleep(message.time);$
				\State $state \gets Close\_Network\_Access(Queue);$
				\State $send\_acknowledgement(state);$
			\EndWhile
		\EndFunction
		\Statex
		\Function{$Close\_Network\_Access$}{$Queue$}
			\State $state.egress \gets Network\_Egress.enqueue(Queue);$
			\State $state.ingress \gets Network\_Ingress.drop\_packets();$
			\State \Return{$state$}
		\EndFunction
		\Statex
		\Function{$Open\_Network\_Access$}{$Queue$}
			\State $state.ingress \gets Network\_Ingress.accept\_packets();$
			\State $state.egress \gets Queue.process\_packets\_to\_network();$
			\State \Return{$state$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}	
		
The tool is composed of four major components:  the window scheduler, ingress controller, egress controller, and the state controller. The window scheduler can be located on any administrative node within the system, preferably co-located with the system job scheduler. The remaining controllers are located throughout the HPC environment, with a copy on each compute node that is designated to execute user applications. 

The system provides enhanced security by time dividing access to the network according to security groups. As jobs are scheduled on the system, the window scheduler must be informed of the intended location and their assigned security group. As jobs are run on compute nodes throughout the system the window scheduler communicates with the state controller on each node to designate time windows. During any individual time window only one security group has authorization to access the network. For any given time window in which a security group does not have access to the network, outgoing network packets are stored in a local queue while incoming packets are just ignored and deleted. The window scheduler is tasked with alternating time window authorization between security groups.

The following subsections describe each components operations in further detail. 
\subsection{State Controller}
The state controller has three major tasks:
\begin{enumerate} \itemsep1pt \parskip0pt \parsep0pt
\item Securely send and receive communication with the window scheduler for the system
\item Transit both the ingress and egress controllers between states of network access and denial
\item Collect and store performance data on the egress queue's memory usage
\end{enumerate}

 

\subsection{Ingress Controller}
The ingress controller is a firewall of incoming network packets and has two states:
\begin{enumerate} \itemsep1pt \parskip0pt \parsep0pt
\item Open access of network packets to applications on the node
\item Closed to network packets except for those from explicitly allowed sources (a whitelist style of firewall)
\end{enumerate}


During the open state incoming packets are processed normally. During the ingress closure, incoming packets, except those allowed by the whitelist, are dropped. The whitelist is designated to allow only necessary infrastructure communications such as Network Time Protocol (NTP), performance measurements, and especially packets from the window scheduler.
\subsection{Egress Controller}
The egress controller, similar to the ingress controller, has two major states:
\begin{enumerate} \itemsep1pt \parskip0pt \parsep0pt
\item Open flow of packets onto the network
\item Diversion of outgoing packets into a blocked queue
\end{enumerate}

\subsection{Window Scheduler}
The window scheduler has three major tasks: 

Determine system network access states for the next time window 
Validate the closure of the previous time window
Communicate the next time window states to compute nodes
To determine the network access states for the next time window the scheduler must run a scheduling algorithm on a few historical inputs. The base case scheduling algorithm is round robin (i.e., equal window size for each security group). To improve performance, a number of heuristics have been considered for the creation of a dynamic priority scheduling algorithm: the egress controller memory usage of compute nodes, number of TCP timeouts, and externally imposed priorities. 
\chapter{Performance}

\begin{figure}
\centerline{\includegraphics[scale=0.8]{TCP_Performance.pdf}}
\caption{TDM effect on TCP application performance.}
\label{fig:TCP_Performance}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.5]{node_state_diagram.pdf}
\caption{State diagram of a compute node.}
\label{fig:node_state_diagram}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.5]{control_state_diagram.pdf}
\caption{State diagram of an example window controller.}
\label{fig:control_state_diagram}
\end{figure}
\subsection{Expected Values}

\begin{table}
	\begin{center}
	    \begin{tabular}{  l | r  }
	    Technical Specifications & \\ \hline
	    CPUs & 16 cores @ 2.2GHz \\
	    Main Memory & 32GB @ DDR3\\
	    \end{tabular}
	\end{center}
	\caption{Technical specifications of compute nodes within the Titan at ORNL.}
	\label{tab:titan_ORNL}
\end{table}

Leading systems in high performance computing can serve as indicators of where commercial HPC systems will be in the upcoming years. The Titan system at Oak Ridge National Laboratories (ORNL) is one such system, the technical details of which are displayed in Table~\ref{tab:titan_ORNL}.

Here we attempt to quantify how the mechanism affects memory usage at the compute node level and effective bandwidth.
\begin{description}
    \item[$T_{on, n}$] Time, within a window, where compute node $n$ has access to the network.
    \item[$T_{window}$] Length in time of a single window.
    \item[$\Pi_{app, n}$] Speed at which the application on compute node $n$ generates network traffic.
    \item[$\Pi_{NIC, n}$] Speed at which the NIC on compute node $n$ can transmit traffic onto the network.
\end{description} 

\subsection{Case Study}
To verify and test the mechanism a test bed was created out of five Dell $1955$ servers seen in Figure~\ref{fig:test_bed}. These servers run Ubuntu server version 12.04 and had their network interfaces configured and connected according to Figure~\ref{fig:tdm_demo_setup}. 

To simulate job execution each node ran a program that sent ICMP (ping) commands at stochastic intervals and speeds to the other member other member of the security group. The mechanism, started and controlled at the control server (see Figure~\ref{fig:test_bed}) alternated between network access for each security group at a time interval of one second of network access per group. A Wireshark (REF PACKET SNIFFING) trace was ran at the control server and the results of which can be seen in Figure~\ref{fig:wireshark} showing a history of packets sent through the network. The black entries represent communications originating from the control server, while the red and blue entries represent packets originating from nodes within the security group. MAKE IMAGE OF RANDOMIZED PINGING AND COMPARE THE BEFORE AFTER - MENTION THIS IS EGRESS INFORMATION, FIND WAY TO SHOW INGRESS.
\begin{figure}
  \begin{center}
    \includegraphics[width=0.9\textwidth]{tdm_demo_setup.pdf}
  \end{center}
  \caption{The network architecture of our demonstration test bed.}
\label{fig:tdm_demo_setup}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.9\textwidth]{test_bed.jpg}
  \end{center}
  \caption{The TDM test bed located in Impact Lab at Arizona State University.}
\label{fig:test_bed}
\end{figure}
    
    
\begin{figure}
  \begin{center}
    \includegraphics[width=0.9\textwidth]{wireshark.jpg}
  \end{center}
  \caption{An example trace of the mechanism's time division property captured using the packet capturing application Wireshark. The colored records represent traffic based from compute nodes of two separate security groups - red and blue.}
\label{fig:wireshark}
\end{figure}

\subsection{Security Validation}
\chapter{Conclusion}
\subsection{Further Work}
Implement dynamic scheduling algorithm on the window scheduler using memory usage statistics from compute nodes. 

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
